{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Model dan Pengujian dengan Gradio di Cloud\n",
    "\n",
    "Notebook ini akan:\n",
    "1. Menginstal dependensi yang diperlukan.\n",
    "2. Membuat file `.jsonl` dari file PDF (jika ada).\n",
    "3. Melakukan fine-tuning model bahasa menggunakan LoRA.\n",
    "4. Menjalankan antarmuka Gradio untuk menguji model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 1: Instalasi Dependensi\n",
    "\n",
    "Jalankan sel ini untuk menginstal semua dependensi yang diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers datasets peft gradio pdfplumber ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 2: Unggah File PDF (Opsional)\n",
    "\n",
    "Jika Anda memiliki file PDF untuk membuat dataset `.jsonl`, unggah file PDF Anda di sini. Jika tidak, lewati langkah ini dan pastikan Anda memiliki file `custom_knowledge.jsonl` yang sudah ada.\n",
    "\n",
    "Catatan: Anda juga dapat mengunggah file `custom_knowledge.jsonl` secara manual jika tidak ingin membuatnya dari PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Buat direktori untuk file PDF\n",
    "pdf_dir = \"./pdfs\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Unggah file PDF atau custom_knowledge.jsonl\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Pindahkan file yang diunggah ke direktori ./pdfs (jika PDF)\n",
    "for filename in uploaded.keys():\n",
    "    if filename.endswith('.pdf'):\n",
    "        os.rename(filename, os.path.join(pdf_dir, filename))\n",
    "        print(f\"File {filename} berhasil diunggah ke {pdf_dir}\")\n",
    "    elif filename == 'custom_knowledge.jsonl':\n",
    "        print(f\"File {filename} berhasil diunggah.\")\n",
    "    else:\n",
    "        print(f\"File {filename} diabaikan (harus .pdf atau custom_knowledge.jsonl)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 3: Membuat File `.jsonl` dari PDF (Opsional)\n",
    "\n",
    "Jalankan sel ini untuk membuat file `custom_knowledge.jsonl` dari file PDF yang diunggah. Jika tidak ada file PDF, langkah ini akan dilewati. Pastikan Ollama sudah terinstal dan model (misalnya `llama3.1`) sudah diunduh.\n",
    "\n",
    "**Catatan**: Langkah ini membutuhkan Ollama untuk menghasilkan pasangan pertanyaan dan jawaban. Jika Anda menjalankan di Colab, Anda perlu menginstal Ollama secara manual (misalnya, melalui `!ollama pull llama3.1`). Namun, karena Colab tidak mendukung Ollama secara langsung, Anda mungkin perlu menjalankan langkah ini di lokal terlebih dahulu untuk membuat file `.jsonl`, lalu mengunggahnya di langkah sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "import ollama\n",
    "\n",
    "# Konfigurasi\n",
    "pdf_directory = \"./pdfs\"\n",
    "output_jsonl = \"custom_knowledge.jsonl\"\n",
    "min_data = 5000\n",
    "ollama_model = \"llama3.1\"\n",
    "\n",
    "# Fungsi untuk membaca teks dari file PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Fungsi untuk menghasilkan pertanyaan dan jawaban menggunakan Ollama\n",
    "def generate_qa(text):\n",
    "    prompt = f\"Generate a question and answer pair based on the following text in English:\\n\\n{text}\\n\\nFormat the output as JSON with 'prompt' and 'completion' keys.\"\n",
    "    response = ollama.chat(model=ollama_model, messages=[{'role': 'user', 'content': prompt}])\n",
    "    try:\n",
    "        qa = json.loads(response['message']['content'])\n",
    "        return qa\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "# Fungsi untuk membuat file .jsonl\n",
    "def create_jsonl(data, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "# Fungsi utama untuk membuat .jsonl\n",
    "def generate_jsonl():\n",
    "    print(f\"Memeriksa file PDF di direktori {pdf_directory}...\")\n",
    "    pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "    if not pdf_files:\n",
    "        print(\"Tidak ada file PDF. Melewati langkah pembuatan .jsonl.\")\n",
    "        return\n",
    "\n",
    "    all_qa = []\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "        text = extract_text_from_pdf(pdf_path)\n",
    "        text_chunks = [text[i:i+500] for i in range(0, len(text), 500)]\n",
    "        for chunk in text_chunks:\n",
    "            qa = generate_qa(chunk)\n",
    "            if qa and 'prompt' in qa and 'completion' in qa:\n",
    "                all_qa.append(qa)\n",
    "\n",
    "    # Ulangi data jika kurang dari 5000\n",
    "    while len(all_qa) < min_data:\n",
    "        all_qa.extend(all_qa[:min_data - len(all_qa)])\n",
    "\n",
    "    create_jsonl(all_qa, output_jsonl)\n",
    "    print(f\"File {output_jsonl} berhasil dibuat dengan {len(all_qa)} entri.\")\n",
    "\n",
    "# Jalankan fungsi\n",
    "generate_jsonl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 4: Fine-Tuning Model\n",
    "\n",
    "Jalankan sel ini untuk melakukan fine-tuning model. Pastikan file `custom_knowledge.jsonl` sudah ada (dibuat di langkah sebelumnya atau diunggah secara manual)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Paksa penggunaan CPU-only\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load model dan tokenizer dengan attn_implementation=\"eager\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "model.to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Konfigurasi LoRA untuk efisiensi\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files=\"custom_knowledge.jsonl\", split='train')\n",
    "\n",
    "# Preprocessing dataset\n",
    "def preprocess_function(examples):\n",
    "    texts = [f\"{p.strip()} [SEP] {c.strip()}\" for p, c in zip(examples[\"prompt\"], examples[\"completion\"])]\n",
    "    tokenized = tokenizer(texts, truncation=True, max_length=256, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments yang dioptimalkan untuk CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=20,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=50,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    eval_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Inisialisasi trainer dengan label_names\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    "    label_names=[\"labels\"],\n",
    ")\n",
    "\n",
    "# Mulai fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Simpan model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "# Gabungkan adapter LoRA dan simpan versi merged\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(\"./fine_tuned_model_merged\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model_merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langkah 5: Jalankan Antarmuka Gradio untuk Pengujian\n",
    "\n",
    "Jalankan sel ini untuk memuat model yang telah di-fine-tune dan menguji model melalui antarmuka Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessor, LogitsProcessorList\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"./fine_tuned_model_merged\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Custom logits processor for stability\n",
    "class SafeLogitsProcessor(LogitsProcessor):\n",
    "    def __call__(self, input_ids, scores):\n",
    "        scores = torch.where(\n",
    "            torch.isnan(scores) | torch.isinf(scores),\n",
    "            torch.full_like(scores, -1e9),\n",
    "            scores\n",
    "        )\n",
    "        return scores\n",
    "\n",
    "# Generate response function\n",
    "def generate_response(prompt, max_new_tokens=100, temperature=0.7):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    logits_processor = LogitsProcessorList([SafeLogitsProcessor()])\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                logits_processor=logits_processor,\n",
    "            )\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return \"Sorry, an error occurred.\"\n",
    "\n",
    "# Chat interface\n",
    "def chat_interface(user_input, history):\n",
    "    if history:\n",
    "        prompt = \"\\n\".join([f\"User: {h[0]}\\nBot: {h[1]}\" for h in history]) + f\"\\nUser: {user_input}\\nBot:\"\n",
    "    else:\n",
    "        prompt = f\"User: {user_input}\\nBot:\"\n",
    "    return generate_response(prompt)\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks(title=\"Chatbot Fine-Tuned Model\") as demo:\n",
    "    gr.Markdown(\"# Chatbot Fine-Tuned Model\\nAsk anything!\")\n",
    "    chatbot = gr.Chatbot(label=\"Conversation\")\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "    submit_btn = gr.Button(\"Send\")\n",
    "    state = gr.State([])\n",
    "\n",
    "    def submit_message(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return \"\", history\n",
    "        bot_response = chat_interface(user_input, history)\n",
    "        history.append((user_input, bot_response))\n",
    "        return \"\", history\n",
    "\n",
    "    submit_btn.click(fn=submit_message, inputs=[user_input, state], outputs=[user_input, chatbot])\n",
    "    user_input.submit(fn=submit_message, inputs=[user_input, state], outputs=[user_input, chatbot])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}